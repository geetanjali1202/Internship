{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be30781",
   "metadata": {},
   "source": [
    "#WEB SCRAPING – ASSIGNMENT 3\n",
    "\n",
    "Internship ID - DS2402\n",
    "\n",
    "Submitted by - Geetanjali Joshi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008508ed",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n",
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n",
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n",
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in.\n",
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n",
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n",
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f3d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in ./anaconda3/lib/python3.10/site-packages (4.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./anaconda3/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in ./anaconda3/lib/python3.10/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in ./anaconda3/lib/python3.10/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in ./anaconda3/lib/python3.10/site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./anaconda3/lib/python3.10/site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: exceptiongroup in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: idna in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sortedcontainers in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./anaconda3/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./anaconda3/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./anaconda3/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6682b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6211be",
   "metadata": {},
   "source": [
    "Q1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de0e7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "338bdeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon: google\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(product):\n",
    "    url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "    data = []\n",
    "    for product in products:\n",
    "        name = product.find('span', class_='a-text-normal').text.strip()\n",
    "        try:\n",
    "            price = product.find('span', class_='a-offscreen').text.strip()\n",
    "        except AttributeError:\n",
    "            price = \"-\"\n",
    "        try:\n",
    "            rating = product.find('span', class_='a-icon-alt').text.strip()\n",
    "        except AttributeError:\n",
    "            rating = \"-\"\n",
    "        try:\n",
    "            reviews_count = product.find('span', class_='a-size-base').text.strip()\n",
    "        except AttributeError:\n",
    "            reviews_count = \"-\"\n",
    "\n",
    "        data.append({\n",
    "            \"Name\": name,\n",
    "            \"Price\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Reviews Count\": reviews_count\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# User input for product\n",
    "user_input = input(\"Enter the product you want to search for on Amazon: \")\n",
    "\n",
    "# Scrape data\n",
    "data = search_amazon_products(user_input)\n",
    "\n",
    "# Print the details of the products found\n",
    "for idx, product in enumerate(data, start=1):\n",
    "    print(f\"Product {idx}:\")\n",
    "    print(f\"Name: {product['Name']}\")\n",
    "    print(f\"Price: {product['Price']}\")\n",
    "    print(f\"Rating: {product['Rating']}\")\n",
    "    print(f\"Reviews Count: {product['Reviews Count']}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb3f8fb",
   "metadata": {},
   "source": [
    "Q2.In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4d9b05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query, max_pages=3):\n",
    "    data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"https://www.flipkart.com/search?q={search_query}&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', class_='_4ddWXP')\n",
    "\n",
    "        if not products:\n",
    "            break\n",
    "\n",
    "        for product in products:\n",
    "            try:\n",
    "                brand_name = product.find('div', class_='_2WkVRV').text\n",
    "            except:\n",
    "                brand_name = \"-\"\n",
    "                \n",
    "            try:\n",
    "                product_name = product.find('a', class_='IRpwTa').text\n",
    "            except:\n",
    "                product_name = \"-\"\n",
    "                \n",
    "            try:\n",
    "                price = product.find('div', class_='_30jeq3 _1_WHN1').text\n",
    "            except:\n",
    "                price = \"-\"\n",
    "                \n",
    "            try:\n",
    "                return_exchange = product.find('div', class_='_3Wnk9T').text\n",
    "            except:\n",
    "                return_exchange = \"-\"\n",
    "                \n",
    "            try:\n",
    "                expected_delivery = product.find('div', class_='_3wS5AW').text\n",
    "            except:\n",
    "                expected_delivery = \"-\"\n",
    "                \n",
    "            try:\n",
    "                availability = product.find('div', class_='_3aV9Tq').text\n",
    "            except:\n",
    "                availability = \"-\"\n",
    "                \n",
    "            try:\n",
    "                product_url = \"https://www.flipkart.com\" + product.find('a', class_='_1fQZEK')['href']\n",
    "            except:\n",
    "                product_url = \"-\"\n",
    "\n",
    "            data.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Name of the Product\": product_name,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_exchange,\n",
    "                \"Expected Delivery\": expected_delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Search query\n",
    "search_query = \"Oneplus Nord\"\n",
    "\n",
    "# Scrape data\n",
    "data = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"flipkart_smartphones_extended.csv\", index=False)\n",
    "\n",
    "print(\"Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4cdcb68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cba669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75e412b0",
   "metadata": {},
   "source": [
    "Q3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4b89de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for 'fruits':\n",
      "\n",
      "Images for 'cars':\n",
      "\n",
      "Images for 'Machine Learning':\n",
      "\n",
      "Images for 'Guitar':\n",
      "\n",
      "Images for 'Cakes':\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def search_google_images(query, num_images=10):\n",
    "    api_key = \"YOUR_API_KEY\"\n",
    "    search_engine_id = \"YOUR_SEARCH_ENGINE_ID\"\n",
    "\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&num={num_images}&searchType=image&key={api_key}&cx={search_engine_id}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    image_urls = []\n",
    "    for item in data.get('items', []):\n",
    "        image_urls.append(item.get('link'))\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    image_urls = search_google_images(keyword)\n",
    "    print(f\"Images for '{keyword}':\")\n",
    "    for idx, url in enumerate(image_urls, start=1):\n",
    "        print(f\"Image {idx}: {url}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44e38e",
   "metadata": {},
   "source": [
    "Q4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a3730cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', class_='_4ddWXP')\n",
    "\n",
    "    data = []\n",
    "    for product in products:\n",
    "        brand_name = product.find('div', class_='_2WkVRV').text.strip()\n",
    "        smartphone_name = product.find('a', class_='IRpwTa').text.strip()\n",
    "\n",
    "        # Default values in case details are missing\n",
    "        color = ram = rom = primary_camera = secondary_camera = display_size = battery_capacity = price = product_url = \"-\"\n",
    "\n",
    "        specifications = product.find_all('li', class_='_21Ahn-')\n",
    "        for spec in specifications:\n",
    "            spec_text = spec.text\n",
    "            if 'Color' in spec_text:\n",
    "                color = spec_text.replace('Color', '').strip()\n",
    "            elif 'RAM' in spec_text:\n",
    "                ram = spec_text.replace('RAM', '').strip()\n",
    "            elif 'ROM' in spec_text:\n",
    "                rom = spec_text.replace('ROM', '').strip()\n",
    "            elif 'Primary Camera' in spec_text:\n",
    "                primary_camera = spec_text.replace('Primary Camera', '').strip()\n",
    "            elif 'Secondary Camera' in spec_text:\n",
    "                secondary_camera = spec_text.replace('Secondary Camera', '').strip()\n",
    "            elif 'Display Size' in spec_text:\n",
    "                display_size = spec_text.replace('Display Size', '').strip()\n",
    "            elif 'Battery Capacity' in spec_text:\n",
    "                battery_capacity = spec_text.replace('Battery Capacity', '').strip()\n",
    "\n",
    "        try:\n",
    "            price = product.find('div', class_='_30jeq3 _1_WHN1').text.strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_url = \"https://www.flipkart.com\" + product.find('a', class_='_1fQZEK')['href']\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        data.append({\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Smartphone Name\": smartphone_name,\n",
    "            \"Color\": color,\n",
    "            \"RAM\": ram,\n",
    "            \"Storage(ROM)\": rom,\n",
    "            \"Primary Camera\": primary_camera,\n",
    "            \"Secondary Camera\": secondary_camera,\n",
    "            \"Display Size\": display_size,\n",
    "            \"Battery Capacity\": battery_capacity,\n",
    "            \"Price\": price,\n",
    "            \"Product URL\": product_url\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Search query\n",
    "search_query = \"Oneplus Nord\"\n",
    "\n",
    "# Scrape data\n",
    "data = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "print(\"Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f90bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeb7e2ef",
   "metadata": {},
   "source": [
    "Q5.Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cf89297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "    \n",
    "    data = []\n",
    "    for laptop in laptops:\n",
    "        details = laptop.find_next('div', class_='right-container')\n",
    "        name = details.find('div', class_='heading-wraper').text.strip()\n",
    "        \n",
    "        specs = details.find_all('div', class_='value')\n",
    "        processor = specs[0].text.strip()\n",
    "        memory = specs[1].text.strip()\n",
    "        os = specs[2].text.strip()\n",
    "        display = specs[3].text.strip()\n",
    "        ram = specs[4].text.strip()\n",
    "        weight = specs[5].text.strip()\n",
    "        dimensions = specs[6].text.strip()\n",
    "        graphic_processor = specs[7].text.strip()\n",
    "        price = details.find('td', class_='smprice').text.strip()\n",
    "        \n",
    "        data.append({\n",
    "            \"Name\": name,\n",
    "            \"Processor\": processor,\n",
    "            \"Memory\": memory,\n",
    "            \"OS\": os,\n",
    "            \"Display\": display,\n",
    "            \"RAM\": ram,\n",
    "            \"Weight\": weight,\n",
    "            \"Dimensions\": dimensions,\n",
    "            \"Graphic Processor\": graphic_processor,\n",
    "            \"Price\": price\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "# Scrape data\n",
    "data = scrape_gaming_laptops(url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "\n",
    "print(\"Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a662d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c98066ee",
   "metadata": {},
   "source": [
    "Q6. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9a50bfd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Find all the rows in the table\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Iterate over each row and extract the required details\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[1;32m     19\u001b[0m   \u001b[38;5;66;03m# Find all the columns in the row\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Forbes website\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the billionaire details\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "# Iterate over each row and extract the required details\n",
    "for row in rows:\n",
    "  # Find all the columns in the row\n",
    "  columns = row.find_all(\"td\")\n",
    "  \n",
    "  # Extract the required details from the columns\n",
    "  rank = columns[0].text.strip()\n",
    "  name = columns[1].text.strip()\n",
    "  net_worth = columns[2].text.strip()\n",
    "  age = columns[3].text.strip()\n",
    "  citizenship = columns[4].text.strip()\n",
    "  source = columns[5].text.strip()\n",
    "  industry = columns[6].text.strip()\n",
    "  \n",
    "  # Print the extracted details\n",
    "  print(\"Rank:\", rank)\n",
    "  print(\"Name:\", name)\n",
    "  print(\"Net Worth:\", net_worth)\n",
    "  print(\"Age:\", age)\n",
    "  print(\"Citizenship:\", citizenship)\n",
    "  print(\"Source:\", source)\n",
    "  print(\"Industry:\", industry)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef78cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c18de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ff12c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5214561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.forbes.com/?sh=44f0e2002254\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "90169167",
   "metadata": {},
   "outputs": [],
   "source": [
    "billionaires = driver.find_elements(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div[3]/div[2]/div[2]/div/div[2]/div[1]/div[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4cf73421",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = []\n",
    "name = []\n",
    "networth = []\n",
    "age = []\n",
    "citizenship = []\n",
    "source = []\n",
    "industry = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d55461d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=124.0.6367.91)\nStacktrace:\n0   chromedriver                        0x00000001047928cc chromedriver + 4368588\n1   chromedriver                        0x000000010478ad60 chromedriver + 4336992\n2   chromedriver                        0x00000001043aec04 chromedriver + 289796\n3   chromedriver                        0x0000000104397d00 chromedriver + 195840\n4   chromedriver                        0x0000000104397c3c chromedriver + 195644\n5   chromedriver                        0x0000000104428d24 chromedriver + 789796\n6   chromedriver                        0x00000001043e5ab4 chromedriver + 514740\n7   chromedriver                        0x00000001043e650c chromedriver + 517388\n8   chromedriver                        0x0000000104756de8 chromedriver + 4124136\n9   chromedriver                        0x000000010475bbd8 chromedriver + 4144088\n10  chromedriver                        0x000000010473c7b0 chromedriver + 4016048\n11  chromedriver                        0x000000010475c508 chromedriver + 4146440\n12  chromedriver                        0x000000010472e264 chromedriver + 3957348\n13  chromedriver                        0x000000010477be50 chromedriver + 4275792\n14  chromedriver                        0x000000010477bfcc chromedriver + 4276172\n15  chromedriver                        0x000000010478a9c0 chromedriver + 4336064\n16  libsystem_pthread.dylib             0x0000000184d46f94 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000184d41d34 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[186], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# scraping ranks\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ranks \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/html/body/div[1]/div[2]/div/div/div[3]/div[2]/div[2]/div/div[2]/div[1]/div[2]/div[1]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ranks:\n\u001b[1;32m      4\u001b[0m     rank1\u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:771\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=124.0.6367.91)\nStacktrace:\n0   chromedriver                        0x00000001047928cc chromedriver + 4368588\n1   chromedriver                        0x000000010478ad60 chromedriver + 4336992\n2   chromedriver                        0x00000001043aec04 chromedriver + 289796\n3   chromedriver                        0x0000000104397d00 chromedriver + 195840\n4   chromedriver                        0x0000000104397c3c chromedriver + 195644\n5   chromedriver                        0x0000000104428d24 chromedriver + 789796\n6   chromedriver                        0x00000001043e5ab4 chromedriver + 514740\n7   chromedriver                        0x00000001043e650c chromedriver + 517388\n8   chromedriver                        0x0000000104756de8 chromedriver + 4124136\n9   chromedriver                        0x000000010475bbd8 chromedriver + 4144088\n10  chromedriver                        0x000000010473c7b0 chromedriver + 4016048\n11  chromedriver                        0x000000010475c508 chromedriver + 4146440\n12  chromedriver                        0x000000010472e264 chromedriver + 3957348\n13  chromedriver                        0x000000010477be50 chromedriver + 4275792\n14  chromedriver                        0x000000010477bfcc chromedriver + 4276172\n15  chromedriver                        0x000000010478a9c0 chromedriver + 4336064\n16  libsystem_pthread.dylib             0x0000000184d46f94 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000184d41d34 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "# scraping ranks\n",
    "ranks = driver.find_elements(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div[3]/div[2]/div[2]/div/div[2]/div[1]/div[2]/div[1]\")\n",
    "for i in ranks:\n",
    "    rank1= i.text\n",
    "    rank.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "80867982",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrank\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName:\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNet Worth:\u001b[39m\u001b[38;5;124m\"\u001b[39m, net_worth)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rank' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7c4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d95cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d6b4fce",
   "metadata": {},
   "source": [
    "Q7. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "623d1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "72df0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.youtube.com/watch?v=RBSUwFGa6Fk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4a64e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments \n",
    "comments = []\n",
    "upvotes = []\n",
    "time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "859be336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping comments\n",
    "comments1 = driver.find_elements(By.XPATH,\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-comments/ytd-item-section-renderer/div[3]/ytd-comment-thread-renderer[10]/ytd-comment-view-model/div[3]/div[2]/ytd-expander/div\")\n",
    "for i in comments1[:10]:\n",
    "    comment= i.text\n",
    "    comments.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "db07244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping for upvotes\n",
    "upvote1 = driver.find_elements(By.XPATH,\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/div/div[2]/div[2]/div/div/ytd-menu-renderer/div[1]/segmented-like-dislike-button-view-model/yt-smartimation/div/div/like-button-view-model/toggle-button-view-model/button-view-model/button/yt-touch-feedback-shape/div/div[2]\")\n",
    "for i in upvote1[:10]:\n",
    "    upvote= i.text\n",
    "    upvotes.append(upvote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2b265ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping for time\n",
    "post_time = driver.find_elements(By.XPATH,\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-comments/ytd-item-section-renderer/div[3]/ytd-comment-thread-renderer[2]/ytd-comment-view-model/div[3]/div[2]/div/div[2]/span[3]/a\")\n",
    "for i in post_time[:10]:\n",
    "    times=i.text\n",
    "    time.append(hostel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "12bbc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment 1\n",
      "comments Wonderful , Simple and Detailed Explanation!\n",
      "upvotes \n",
      "time Wombat's City Hostel London\n",
      "\n",
      "\n",
      "comment 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcomments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupvotes\u001b[39m\u001b[38;5;124m\"\u001b[39m, upvotes[i])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, time[i])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"comment\", i+1)\n",
    "    print(\"comments\", comments[i])\n",
    "    print(\"upvotes\", upvotes[i])\n",
    "    print(\"time\", time[i])\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fffedc",
   "metadata": {},
   "source": [
    "Q8.Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e1cf2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3060dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.hostelworld.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "04693e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[2]/div/div/div/div[2]/input\")\n",
    "search1.send_keys('london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd118e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div/div/div[2]/input\")\n",
    "search.send_keys('london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dd23dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search bar \n",
    "search2 = search = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[3]/div/div/div[2]/div/div[1]/div/div[2]/div/ul/li[2]/button\")\n",
    "search2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ad00993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[3]/div/div/div[2]/div/div[2]/div/div[3]/div/div/div/div[1]/div[2]/div[35]/button/div\")\n",
    "checkin.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b82cbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkout = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[3]/div/div/div[2]/div/div[2]/div/div[3]/div/div/div/div[1]/div[2]/div[37]/button/div\")\n",
    "checkout.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "17750dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next_page =  driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[3]/div/div/div[3]/div/div/button[2]\")\n",
    "checkin.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14022c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostel_name = []\n",
    "distance_from_city_centre = []\n",
    "ratings = []\n",
    "total_reviews = []\n",
    "overall_reviews = []\n",
    "privates_from_price = []\n",
    "dorms_from_price = []\n",
    "facilities = []\n",
    "property_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a36a59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping for hostel name\n",
    "hotels = driver.find_elements(By.XPATH,\"/html/body/div[3]/div/div/div[2]/div[2]/div[1]/div/div/div[5]/div[1]/a/a/div[2]/div[1]/div[2]/span\")\n",
    "for hostel in hotels[:10]:\n",
    "    hostel=hostel.text\n",
    "    hostel_name.append(hostel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "590d1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping for distance from the city\n",
    "distances = driver.find_elements(By.XPATH,\"/html/body/div[3]/div/div/div[2]/div[2]/div[1]/div/div/div[5]/div[1]/a/a/div[2]/div[2]/span[2]\")\n",
    "for i in distances[:10]:\n",
    "    distance=i.text\n",
    "    distance_from_city_centre.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "580a8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping data for rating\n",
    "ratings_tag = driver.find_elements(By.XPATH, '/html/body/div[3]/div/div/div[2]/div[2]/div[1]/div/div/div[5]/div[1]/a/a/div[2]/div[1]/div[3]/div/div[1]/span[1]')\n",
    "for i in ratings_tag[:10]:\n",
    "    rating =i.text\n",
    "    ratings.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1b1ecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping data for total_reviews\n",
    "reviews = driver.find_elements(By.XPATH, '/html/body/div[3]/div/div/div[2]/section/div[10]/div/div[1]/div[1]/div[2]/div[2]')\n",
    "for i in reviews[:10]:\n",
    "    review =i.text\n",
    "    total_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a90fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping data for dorms_from_price\n",
    "dorms_prices = driver.find_elements(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div[1]/div/div/div[5]/div[1]/a/a/div[2]/div[4]/div/div[2]/div[3]')\n",
    "for i in dorms_prices[:10]:\n",
    "    dprice = i.text\n",
    "    dorms_from_price.append(dprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c854e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping data for facilities\n",
    "facilities1 = driver.find_elements(By.XPATH,'/html/body/div[3]/div/div/div[2]/section/div[5]/div/nav/ul/li[2]/a/div')\n",
    "for i in facilities1[:10]:\n",
    "    facility = i.text\n",
    "    facilities.append(facility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a34f2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarping data for dorms_from_price\n",
    "descriptions = driver.find_elements(By.XPATH,'/html/body/div[3]/div/div/div[2]/section/div[10]/div/div[2]/div[6]/div/div[2]')\n",
    "for i in descriptions[:10]:\n",
    "    description = i.text\n",
    "    property_description.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Hostel':hostel_name ,'Distance from city':distance_from_city_centre,'Ratings':ratings,'Total reviews':total_reviews,'Overall Reviews':overall_reviews,'Dorm prices':dorms_from_price,'Facilities': facilities,'Property description':property_description})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598d3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
